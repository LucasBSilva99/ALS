# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o-EPqBTHVqVEMfvXDficqw8l_veQ7eWp
"""

import torch
import torch.nn as nn
import torchvision
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import math

from torch.nn.init import kaiming_uniform_
from torch.nn.init import xavier_uniform_
import torchmetrics
from torchmetrics import Accuracy

import pytorch_lightning as pl
from pytorch_lightning import Trainer

from torch.optim.lr_scheduler import ReduceLROnPlateau

# neural nets
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
import tensorflow_addons as tfa
import tensorflow_probability as tfp

#useful
from sklearn.metrics import roc_auc_score
from tqdm.notebook import tqdm
from typing import Union, Optional
from time import time
import warnings
warnings.filterwarnings('ignore')
from pylab import rcParams
from matplotlib import rcParams
from matplotlib.ticker import MaxNLocator
from sklearn.preprocessing import LabelEncoder
from multiprocessing import cpu_count
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import  numpy as np
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import KNNImputer
import matplotlib.pyplot as plt

from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

import keras
from keras.layers import LSTM, Dropout, Dense
import tensorflow as tf

from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold, train_test_split
from sklearn.metrics import roc_auc_score

import pymrmr

from sklearn.ensemble import ExtraTreesClassifier

def tree_classifier_selec(X, y, n):
  model = ExtraTreesClassifier(n_estimators=10)
  model.fit(X, y)

  #plot graph of feature importances for better visualization
  feat_importances = pd.Series(model.feature_importances_, index=X.columns)
  print(feat_importances.nlargest(n))
  feat_importances.nlargest(n).plot(kind='barh')
  plt.show()

from sklearn.feature_selection import f_regression

# inputs:
#    X: pandas.DataFrame, features
#    y: pandas.Series, target variable
#    K: number of features to select

def f_regression_selec(X, y, k):

  # compute F-statistics and initialize correlation matrix
  F = pd.Series(f_regression(X, y['Evolution'])[0], index = X.columns)
  corr = pd.DataFrame(.00001, index = X.columns, columns = X.columns)
  K = k
  # initialize list of selected features and list of excluded features
  selected = []
  not_selected = X.columns.to_list()

  # repeat K times
  for i in range(K):
    
      # compute (absolute) correlations between the last selected feature and all the (currently) excluded features
      if i > 0:
          last_selected = selected[-1]
          corr.loc[not_selected, last_selected] = X[not_selected].corrwith(X[last_selected]).abs().clip(.00001)
          
      # compute FCQ score for all the (currently) excluded features (this is Formula 2)
      score = F.loc[not_selected] / corr.loc[not_selected, selected].mean(axis = 1).fillna(.00001)
      
      # find best feature, add it to selected and remove it from not_selected
      best = score.index[score.argmax()]
      selected.append(best)
      not_selected.remove(best)

      return selected, not_selected

def feature_corr(X):
  #get correlations of each features in dataset
  corrmat = X.corr()
  top_corr_features = corrmat.index
  plt.figure(figsize=(20,20))
  #plot heat map
  g=sns.heatmap(X[top_corr_features].corr(),annot=True,cmap="RdYlGn")